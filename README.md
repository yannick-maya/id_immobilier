# ğŸ  ID Immobilier â€” Indice Intelligent du MarchÃ© Immobilier au Togo

> Projet Big Data | Collecte, nettoyage, analyse et visualisation des prix immobiliers au Togo

---

## ğŸ“Œ Contexte

Au Togo, le marchÃ© immobilier manque de transparence : les prix sont estimÃ©s informellement,
les annonces dispersÃ©es sur plusieurs plateformes, et les donnÃ©es officielles peu exploitÃ©es.
**ID Immobilier** construit un pipeline de donnÃ©es qui agrÃ¨ge plusieurs sources pour produire
un indice fiable du prix au mÂ² par zone gÃ©ographique.

---

## ğŸ—‚ï¸ Sources de donnÃ©es

| Source | Type | Lignes | Description |
|--------|------|--------|-------------|
| ImmoAsk | Annonces web | 500 | Plateforme immobiliÃ¨re togolaise |
| Facebook Marketplace | Annonces rÃ©seaux sociaux | 80 | Annonces scrappÃ©es |
| CoinAfrique | Annonces web | 4 844 | Plateforme panafricaine |
| Valeurs VÃ©nales OTR | DonnÃ©es officielles | 354 | Prix cadastraux officiels Togo |

---

## ğŸ—ï¸ Architecture Big Data

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SOURCES DE DONNÃ‰ES                    â”‚
â”‚  ImmoAsk â”‚ Facebook â”‚ CoinAfrique â”‚ Valeurs VÃ©nales OTR  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              APACHE AIRFLOW (Orchestration)              â”‚
â”‚         DAG hebdomadaire â€” dag_immobilier.py             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â–¼            â–¼            â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  data/raw/   â”‚  â”‚  APACHE SPARK        â”‚
  â”‚  (CSV bruts) â”‚â†’ â”‚  (PySpark Cleaning)  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   data/cleaned/      â”‚
                    â”‚   (CSV + Parquet)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   MySQL Database     â”‚
                    â”‚   id_immobilier      â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                    â”‚  â”‚ source_donnees â”‚  â”‚
                    â”‚  â”‚ zone_geo       â”‚  â”‚
                    â”‚  â”‚ bien_immob     â”‚  â”‚
                    â”‚  â”‚ annonce        â”‚  â”‚
                    â”‚  â”‚ stats_zone     â”‚  â”‚
                    â”‚  â”‚ indice_immo    â”‚  â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  STREAMLIT Dashboard â”‚
                    â”‚  Prix au mÂ² / Zone   â”‚
                    â”‚  Indice ID Immob.    â”‚
                    â”‚  Cartes Folium       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Structure du projet

```
id_immobilier/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/         â† Fichiers sources CSV/Excel originaux
â”‚   â”œâ”€â”€ cleaned/     â† DonnÃ©es nettoyÃ©es (CSV + Parquet)
â”‚   â””â”€â”€ gold/        â† DonnÃ©es agrÃ©gÃ©es finales
â”œâ”€â”€ notebooks/       â† Exploration Jupyter
â”‚   â””â”€â”€ exploration.ipynb
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ ingestion.py    â† Lecture des sources Excel â†’ CSV
â”‚   â”œâ”€â”€ cleaning.py     â† Nettoyage PySpark
â”‚   â”œâ”€â”€ modeling.py     â† Insertion dans MySQL
â”‚   â”œâ”€â”€ indicators.py   â† Calcul prix mÂ², stats par zone
â”‚   â””â”€â”€ index.py        â† Calcul indice immobilier
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ dag_immobilier.py  â† DAG Airflow hebdomadaire
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ app.py          â† Dashboard Streamlit
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ schema.sql      â† SchÃ©ma MySQL complet
â”œâ”€â”€ .env.example        â† Variables d'environnement
â”œâ”€â”€ requirements.txt    â† DÃ©pendances Python
â””â”€â”€ README.md
```

---

## ğŸš€ Installation et lancement

### 1. Cloner et installer

```bash
git clone https://github.com/ton-username/id_immobilier.git
cd id_immobilier
pip install -r requirements.txt
```

### 2. Configurer MySQL

```bash
cp .env.example .env
# Ã‰dite .env avec tes identifiants MySQL
```

```bash
mysql -u root -p < sql/schema.sql
```

### 3. Placer les fichiers sources

```bash
# Copie tes 4 fichiers Excel dans :
data/raw/sources/
```

### 4. Lancer le pipeline manuellement

```bash
python pipeline/ingestion.py
spark-submit --master local[*] pipeline/cleaning.py
python pipeline/modeling.py
python pipeline/indicators.py
python pipeline/index.py
```

### 5. Lancer le dashboard

```bash
streamlit run dashboard/app.py
```

### 6. (Optionnel) Lancer Airflow

```bash
airflow db init
airflow webserver --port 8080
airflow scheduler
# Puis active le DAG "id_immobilier_pipeline" dans l'interface
```

---

## ğŸ“Š Indicateurs produits

- Prix moyen au mÂ² par zone et type de bien
- Prix mÃ©dian au mÂ² par zone
- Ã‰cart entre prix de marchÃ© et valeurs vÃ©nales officielles
- Indice immobilier ID Immobilier (Base 100)
- Tendances : HAUSSE / STABLE / BAISSE par zone

---

## ğŸ‘¨â€ğŸ’» Technologies utilisÃ©es

| Couche | Technologie |
|--------|------------|
| Ingestion | Python, pandas, openpyxl |
| Nettoyage | Apache Spark (PySpark) |
| Orchestration | Apache Airflow |
| Stockage | MySQL, Parquet |
| Analyse | pandas, SQL |
| Visualisation | Streamlit, Plotly, Folium |

---

## ğŸ“ Projet acadÃ©mique

Cours : Introduction au Big Data  
Encadrant : [Nom du professeur]  
DonnÃ©es : ImmoAsk, Facebook Marketplace, CoinAfrique, OTR Togo


  todo


  
# 1. Aller a la racine du projet
cd C:\Users\yanni\Desktop\Projet_Immobilier\id_immobilier

# 2. Lancer tous les conteneurs
docker-compose up -d

# 3. Verifier que tout tourne
docker-compose ps

# 4. Voir les logs si probleme
docker-compose logs -f airflow
docker-compose logs -f streamlit
```

---

## URLs apres lancement
```
Dashboard Streamlit  : http://localhost:8501
Airflow              : http://localhost:8081  (admin / admin1234)
Spark UI             : http://localhost:8080
MySQL                : localhost:3307
```

---

## Point important â€” XAMPP et MySQL

Comme tu utilises XAMPP sur le port 3306, le MySQL Docker est configure sur le **port 3307** pour eviter le conflit. Quand tu utilises Docker, change ton `.env` :
```
MYSQL_HOST=localhost
MYSQL_USER=immo_user
MYSQL_PASSWORD=immo1234
MYSQL_DB=id_immobilier
# Port 3307 pour Docker, 3306 pour XAMPP

# SPARK code

docker exec -it id_immobilier_spark /opt/spark/bin/spark-submit --master spark://spark:7077 /app/pipeline/cleaning_v2.py


PS C:\Users\yanni\Desktop\Projet_Immobilier\id_immobilier> docker exec -it id_immobilier_spark /opt/spark/bin/spark-submit --master spark://spark:7077 /app/pipeline/cleaning_v2.py
26/02/19 17:11:35 INFO SparkContext: Running Spark version 3.5.1
26/02/19 17:11:35 INFO SparkContext: OS info Linux, 6.6.87.2-microsoft-standard-WSL2, amd64
26/02/19 17:11:35 INFO SparkContext: Java version 11.0.22
Nettoyage V2 en cours...

Traitement : immoask
  Avant nettoyage : 460 lignes
  Apres nettoyage : 388 lignes valides
  Rejetes         : 72 lignes
  Repartition des rejets :
+------------------+-----+
|raison_rejet      |count|
+------------------+-----+
|prix_m2_trop_bas  |68   |
|prix_m2_trop_eleve|3    |
|zone_invalide     |1    |
+------------------+-----+


Traitement : facebook
  Avant nettoyage : 89 lignes
  Apres nettoyage : 31 lignes valides
  Rejetes         : 58 lignes
  Repartition des rejets :
+------------------------+-----+
|raison_rejet            |count|
+------------------------+-----+
|zone_invalide           |41   |
|prix_ou_surface_manquant|13   |
|prix_m2_trop_bas        |4    |
+------------------------+-----+


Traitement : coinafrique
  Avant nettoyage : 4841 lignes
  Apres nettoyage : 3572 lignes valides
  Rejetes         : 1269 lignes
  Repartition des rejets :
+------------------------+-----+
|raison_rejet            |count|
+------------------------+-----+
|prix_ou_surface_manquant|975  |
|prix_m2_trop_bas        |106  |
|zone_invalide           |70   |
|zone_description_lieu   |66   |
|zone_trop_longue        |35   |
|prix_m2_trop_eleve      |17   |
==================================================
Total avant nettoyage : 5390
Total valides         : 3991
Total rejetes         : 1399
Taux de rejet         : 25.96%
Valeurs venales       : 354

Valides  -> /app/data/cleaned_v2/annonces
Rejetes  -> /app/data/raw/rejets/annonces_rejetees
==================================================
PS C:\Users\yanni\Desktop\Projet_Immobilier\id_immobilier>










# 1. Lire les fichiers Excel et convertir en CSV
python pipeline/ingestion.py

# 2. Nettoyer avec Spark (filtrage + standardisation)
python pipeline/cleaning_v2.py

# 3. InsÃ©rer dans MySQL
python pipeline/modeling_v2.py

# 4. Calculer les statistiques par zone
python pipeline/indicators.py

# 5. Calculer l'indice immobilier
python pipeline/index.py

# 6. Lancer le dashboard
streamlit run dashboard/app.py
```

---

## Pourquoi cet ordre ?
```
ingestion     â†’ produit les CSV dans data/raw/
cleaning_v2   â†’ lit data/raw/  â†’ produit data/cleaned_v2/
modeling_v2   â†’ lit data/cleaned_v2/ â†’ remplit MySQL       
      <!-- Avant (donnÃ©es brutes)  : 3 993 annonces | 28% rejet
      Apres (donnÃ©es camarade): 4 540 annonces |  5% rejet -->
indicators    â†’ lit MySQL (table annonce) â†’ remplit statistiques_zone
index         â†’ lit statistiques_zone â†’ remplit indice_immobilier
streamlit     â†’ lit tout MySQL â†’ affiche le dashboard

# airflow

docker exec -it id_immobilier_airflow airflow users create --username admin --password admin1234 --firstname Admin --lastname User --role Admin --email admin@idimmobilier.tg